{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Analysis\n",
    "\n",
    "Visualize and interpret training runs for the StepMania difficulty classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = Path('../checkpoints')\n",
    "\n",
    "# List available checkpoints\n",
    "checkpoints = list(CHECKPOINT_DIR.glob('*.pt'))\n",
    "print(\"Available checkpoints:\")\n",
    "for cp in sorted(checkpoints):\n",
    "    print(f\"  {cp.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "checkpoint_path = CHECKPOINT_DIR / 'best_val_loss.pt'\n",
    "if not checkpoint_path.exists():\n",
    "    checkpoint_path = CHECKPOINT_DIR / 'last.pt'\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "print(f\"Loaded: {checkpoint_path.name}\")\n",
    "print(f\"Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "print(f\"Best val loss: {checkpoint.get('best_val_loss', 'N/A'):.4f}\")\n",
    "\n",
    "history = checkpoint.get('history', {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if history:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss\n",
    "    ax = axes[0]\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    ax.plot(epochs, history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "    ax.plot(epochs, history['val_loss'], 'r-', label='Validation', linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training and Validation Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Find best epoch\n",
    "    best_epoch = np.argmin(history['val_loss']) + 1\n",
    "    best_val_loss = min(history['val_loss'])\n",
    "    ax.axvline(best_epoch, color='green', linestyle='--', alpha=0.7, label=f'Best (epoch {best_epoch})')\n",
    "    ax.scatter([best_epoch], [best_val_loss], color='green', s=100, zorder=5)\n",
    "    \n",
    "    # Accuracy\n",
    "    ax = axes[1]\n",
    "    ax.plot(epochs, history['train_acc'], 'b-', label='Train', linewidth=2)\n",
    "    ax.plot(epochs, history['val_acc'], 'r-', label='Validation', linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Training and Validation Accuracy')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    best_val_acc = max(history['val_acc'])\n",
    "    best_acc_epoch = np.argmax(history['val_acc']) + 1\n",
    "    ax.axvline(best_acc_epoch, color='green', linestyle='--', alpha=0.7)\n",
    "    ax.scatter([best_acc_epoch], [best_val_acc], color='green', s=100, zorder=5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Best validation loss: {best_val_loss:.4f} at epoch {best_epoch}\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f} at epoch {best_acc_epoch}\")\n",
    "else:\n",
    "    print(\"No history found in checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Overfitting Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if history and len(history['train_loss']) > 5:\n",
    "    # Compute generalization gap\n",
    "    train_loss = np.array(history['train_loss'])\n",
    "    val_loss = np.array(history['val_loss'])\n",
    "    gap = val_loss - train_loss\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    epochs = range(1, len(gap) + 1)\n",
    "    ax.plot(epochs, gap, 'purple', linewidth=2)\n",
    "    ax.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax.fill_between(epochs, 0, gap, where=(gap > 0), color='red', alpha=0.3, label='Overfitting')\n",
    "    ax.fill_between(epochs, 0, gap, where=(gap <= 0), color='green', alpha=0.3, label='Underfitting')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Generalization Gap (Val - Train)')\n",
    "    ax.set_title('Overfitting Analysis')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary\n",
    "    final_gap = gap[-1]\n",
    "    if final_gap > 0.5:\n",
    "        print(f\"WARNING: Significant overfitting detected (gap: {final_gap:.3f})\")\n",
    "        print(\"Consider: more dropout, data augmentation, or early stopping\")\n",
    "    elif final_gap < -0.1:\n",
    "        print(f\"Model may be underfitting (gap: {final_gap:.3f})\")\n",
    "        print(\"Consider: larger model, more epochs, or lower regularization\")\n",
    "    else:\n",
    "        print(f\"Good generalization (gap: {final_gap:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Learning Rate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if history:\n",
    "    # Compute smoothed loss derivative to detect learning rate issues\n",
    "    train_loss = np.array(history['train_loss'])\n",
    "    \n",
    "    # Simple moving average\n",
    "    window = min(5, len(train_loss) // 3)\n",
    "    if window > 1:\n",
    "        smoothed = np.convolve(train_loss, np.ones(window)/window, mode='valid')\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.plot(range(1, len(train_loss) + 1), train_loss, 'b-', alpha=0.3, label='Raw')\n",
    "        ax.plot(range(window, len(train_loss) + 1), smoothed, 'b-', linewidth=2, label='Smoothed')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Training Loss')\n",
    "        ax.set_title('Training Loss (Smoothed)')\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Check for plateaus\n",
    "        recent_change = (smoothed[-1] - smoothed[-min(5, len(smoothed))]) / smoothed[-min(5, len(smoothed))]\n",
    "        if abs(recent_change) < 0.01:\n",
    "            print(\"Training has plateaued - LR reduction may have kicked in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrix (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = checkpoint.get('confusion_matrix', None)\n",
    "\n",
    "if confusion_matrix is not None:\n",
    "    if isinstance(confusion_matrix, torch.Tensor):\n",
    "        confusion_matrix = confusion_matrix.cpu().numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Normalize by row (true labels)\n",
    "    cm_normalized = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=range(1, 11), yticklabels=range(1, 11), ax=ax)\n",
    "    ax.set_xlabel('Predicted Difficulty')\n",
    "    ax.set_ylabel('True Difficulty')\n",
    "    ax.set_title('Normalized Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    per_class_acc = np.diag(cm_normalized)\n",
    "    print(\"\\nPer-class accuracy:\")\n",
    "    for i, acc in enumerate(per_class_acc):\n",
    "        bar = '#' * int(acc * 20)\n",
    "        print(f\"  Difficulty {i+1:2d}: {acc:.2f} {bar}\")\n",
    "else:\n",
    "    print(\"No confusion matrix in checkpoint.\")\n",
    "    print(\"Run validation with confusion matrix tracking to generate one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if history:\n",
    "    n_epochs = len(history['train_loss'])\n",
    "    final_train_loss = history['train_loss'][-1]\n",
    "    final_val_loss = history['val_loss'][-1]\n",
    "    final_train_acc = history['train_acc'][-1]\n",
    "    final_val_acc = history['val_acc'][-1]\n",
    "    best_val_loss = min(history['val_loss'])\n",
    "    best_val_acc = max(history['val_acc'])\n",
    "    \n",
    "    print(f\"Epochs completed: {n_epochs}\")\n",
    "    print(f\"\")\n",
    "    print(f\"Final metrics:\")\n",
    "    print(f\"  Train loss: {final_train_loss:.4f}\")\n",
    "    print(f\"  Val loss:   {final_val_loss:.4f}\")\n",
    "    print(f\"  Train acc:  {final_train_acc:.4f}\")\n",
    "    print(f\"  Val acc:    {final_val_acc:.4f}\")\n",
    "    print(f\"\")\n",
    "    print(f\"Best metrics:\")\n",
    "    print(f\"  Val loss:   {best_val_loss:.4f}\")\n",
    "    print(f\"  Val acc:    {best_val_acc:.4f}\")\n",
    "    print(f\"\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"Recommendations:\")\n",
    "    if final_val_acc < 0.3:\n",
    "        print(\"  - Model struggling. Check data quality and class balance.\")\n",
    "    elif final_val_acc < 0.5:\n",
    "        print(\"  - Moderate performance. Consider longer training or architecture changes.\")\n",
    "    elif final_val_acc < 0.7:\n",
    "        print(\"  - Good progress. Fine-tune hyperparameters for better results.\")\n",
    "    else:\n",
    "        print(\"  - Strong performance! Consider ensemble or test set evaluation.\")\n",
    "    \n",
    "    if final_val_loss > final_train_loss * 1.5:\n",
    "        print(\"  - Overfitting detected. Add regularization or data augmentation.\")\n",
    "else:\n",
    "    print(\"No training history available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Architecture Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model_state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in state_dict.values())\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"\")\n",
    "    print(\"Layer sizes:\")\n",
    "    for name, param in state_dict.items():\n",
    "        if 'weight' in name:\n",
    "            print(f\"  {name}: {list(param.shape)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
